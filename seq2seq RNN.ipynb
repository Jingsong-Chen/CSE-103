{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import spacy\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the seq2seq RNN sturcture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the encoder that generates the context vector of the sentences\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout      \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)     \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)   \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    " \n",
    "\n",
    "    def forward(self, src):\n",
    "        \n",
    "        # src shape: [longest src sent len, batch size]\n",
    "        # the source is still a batch of sentences\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))  # in this step, each word is converted from a number to a vector\n",
    "        \n",
    "        # embedded shape: [longest src sent len, batch size, emb dim]\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        # the rnn will do a forward propagation on each embedded sentence\n",
    "        \n",
    "        # outputs shape: [src sent len, batch size, hid dim * n directions]\n",
    "        # hidden shape: [n layers * n directions, batch size, hid dim]\n",
    "        # cell shape: [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        # outputs are always from the top hidden layer. since this is an encoder, it is discarded\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buid the decoder that deciphers the context vectors to source sentences\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = output_dim  # this is a probability distribution over all the vocabs\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout       \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)     \n",
    "        self.out = nn.Linear(hid_dim, output_dim)     \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "  \n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        # input shape: [batch size]\n",
    "        # hidden shape: [n layers * n directions, batch size, hid dim]\n",
    "        # cell shape: [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        # n directions in the decoder will always be 1, therefore:\n",
    "        # hidden shape: [n layers, batch size, hid dim]\n",
    "        # context shape: [n layers, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        # input shape: [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded shape: [1, batch size, emb dim]\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        # output shape: [sent len, batch size, hid dim * n directions]\n",
    "        # hidden shape: [n layers * n directions, batch size, hid dim]\n",
    "        # cell shape: [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        # sent len and n directions will always be 1 in the decoder, therefore:\n",
    "        # output shape: [1, batch size, hid dim]\n",
    "        # hidden shape: [n layers, batch size, hid dim]\n",
    "        # cell shape: [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.out(output.squeeze(0))\n",
    "        \n",
    "        # prediction shape: [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a sequence to sequence network that train the encoders to produce context vectors that represent the sentences well\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "    \n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        # src shape: [longest src sent len, batch size]\n",
    "        # trg shape: [longest trg sent len, batch size]\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing (correct word)\n",
    "        # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]  # the number of sentences in the batch\n",
    "        max_len = trg.shape[0]  # length of the longest sentence in the batch\n",
    "        trg_vocab_size = self.decoder.output_dim  # literal meaning of the variable\n",
    "        \n",
    "        # tensor to store decoder outputs. initialized with all 0s\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        # last hidden states for all sentences of the encoder\n",
    "        # they are used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)  # this triggers the forward function defined in Encoder class\n",
    "        context_vectors = hidden.data.clone()  # this is a list of context vectors generated by each encoder layer\n",
    "        \n",
    "        # the decoder netword doesn't predict sentence by sentence, but word by word\n",
    "        # it predicts the 1st word for all sentences, then the 2nd word for all sentences, and so on\n",
    "        \n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]  # get all the first characters from the sentences, which are all <sos>\n",
    "        \n",
    "        # max_len is the length of the longest sentence in the batch\n",
    "        for t in range(1, max_len):  \n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)  # invoke the decoder forward function\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]  # choose the word based on the probability distribution\n",
    "            input = (trg[t] if teacher_force else top1)  # update the input. sometimes use the ground truth\n",
    "        \n",
    "        return outputs, context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the helper functions that interact with the seq2seq network\n",
    "\n",
    "# initialize the weights with normal distribution\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "\n",
    "# count the number of trainable parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# train\n",
    "def train(model, iterator, optimizer, criterion, clip):   \n",
    "    model.train()   \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # the enumerator partitions all the sentences into small batches\n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src  # all the source sentences in a batch\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # train the model on src sentences and target sentences\n",
    "        # this calls the forward function defined in the model(seq2seq) class\n",
    "        output, context_vectors = model(src, trg)\n",
    "        \n",
    "        # the shape of trg: [trg sent len, batch size]\n",
    "        # output = [trg sent len, batch size, output dim]\n",
    "        \n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        # trg = [(trg sent len - 1) * batch size]\n",
    "        #output = [(trg sent len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# evaluation\n",
    "def evaluate(model, iterator, criterion):   \n",
    "    model.eval()   \n",
    "    epoch_loss = 0 \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output, context_vectors = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            # trg shape: [trg sent len, batch size]\n",
    "            # output shape: [trg sent len, batch size, output dim]\n",
    "\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            # trg shape: [(trg sent len - 1) * batch size]\n",
    "            # output shape: [(trg sent len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), context_vectors\n",
    "\n",
    "\n",
    "# epoch timing\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path variables\n",
    "BASE_DIR = os.path.abspath('')\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "REQUEST_DATA_PATH = os.path.join(DATA_DIR, 'clean_requests.pickle')\n",
    "TEXT_LABEL_PATH = os.path.join(DATA_DIR, 'twitter-airline-sentiment/Tweets.csv')\n",
    "TRAIN_EN_PATH = os.path.join(DATA_DIR, 'multi30k/train.en')\n",
    "VAL_EN_PATH = os.path.join(DATA_DIR, 'multi30k/val.en')\n",
    "TEST_EN_PATH = os.path.join(DATA_DIR, 'multi30k/test2016.en')\n",
    "# TRAIN_SRC_PATH = os.path.join(DATA_DIR, 'multi30k/train.src')\n",
    "# VAL_SRC_PATH = os.path.join(DATA_DIR, 'multi30k/val.src')\n",
    "# TEST_SRC_PATH = os.path.join(DATA_DIR, 'multi30k/test2016.src')\n",
    "TRAIN_TOKEN_PATH = os.path.join(DATA_DIR, 'tokens.pickle')\n",
    "CONTEXT_VECTORS_PATH = os.path.join(DATA_DIR, 'context_vectors.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seeds for deterministic results\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tokenizes English text from a string into a list of strings (tokens)\n",
    "\"\"\"\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the source text and target text of the seq2seq network\n",
    "# use 'tokenize_en' defined above as the tokenizer\n",
    "# <sos>: start of the sentence\n",
    "# <eos>: end of the sentence\n",
    "# lower case char only\n",
    "SRC = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14640 rows and 2 coloumns in the data set.\n",
      "The columns are: ['airline_sentiment', 'text']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>plus you've added commercials to the experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>I didn't today... Must mean I need to take ano...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>it's really aggressive to blast obnoxious \"ent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>and it's a really big bad thing about it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                               What @dhepburn said.\n",
       "1          positive  plus you've added commercials to the experienc...\n",
       "2           neutral  I didn't today... Must mean I need to take ano...\n",
       "3          negative  it's really aggressive to blast obnoxious \"ent...\n",
       "4          negative           and it's a really big bad thing about it"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data from the CSV file. use only two columns\n",
    "data = pd.read_csv(TEXT_LABEL_PATH, usecols=['airline_sentiment', 'text'])\n",
    "\n",
    "# drop that '@VirginAmerica' at the beginning of every sentence\n",
    "for index, row in data.iterrows():\n",
    "    row['text'] = row['text'].replace('@VirginAmerica ', '')\n",
    "\n",
    "# view the stats of the dataframe\n",
    "print(\"There are\", data.shape[0], \"rows and\", data.shape[1], \"coloumns in the data set.\")\n",
    "print(\"The columns are:\", list(data.columns))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14640"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(data['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the text data (an array of strings)\n",
    "comments = list(data['text'])[:50]  # change it to normal size later! \n",
    "# requests = pickle.load(open(REQUEST_DATA_PATH, \"rb\"))\n",
    "# requests = requests[:50]\n",
    "\n",
    "# devide the text data into training, validating, and testing set\n",
    "# random.shuffle(comments)\n",
    "comment_number = len(comments)\n",
    "test_text = comments[:int(comment_number*0.03)]\n",
    "validation_text = comments[int(comment_number*0.03):int(comment_number*0.06)]\n",
    "train_text = comments[int(comment_number*0.06):]\n",
    "#random.shuffle(requests)\n",
    "# request_number = len(requests)\n",
    "# test = requests[:int(request_number*0.03)]\n",
    "# validation = requests[int(request_number*0.03):int(request_number*0.06)]\n",
    "# train = requests[int(request_number*0.06):]\n",
    "\n",
    "# save the data for Multi30k\n",
    "with open(TEST_EN_PATH, 'w') as target:\n",
    "    for t in test_text:\n",
    "        line = t + '\\n'\n",
    "        target.write(line)\n",
    "target.close()\n",
    "with open(VAL_EN_PATH, 'w') as target:\n",
    "    for v in validation_text:\n",
    "        line = v + '\\n'\n",
    "        target.write(line)\n",
    "target.close()\n",
    "with open(TRAIN_EN_PATH, 'w') as target:\n",
    "    for t in train_text:\n",
    "        line = t + '\\n'\n",
    "        target.write(line)\n",
    "target.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 48\n",
      "Number of validation examples: 2\n",
      "Number of testing examples: 1\n"
     ]
    }
   ],
   "source": [
    "# tokenize the text\n",
    "# exts = ('.en', '.en'): source and target are both in English\n",
    "# fields = (SRC, TRG): source and target are actually the same\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.en', '.en'), \n",
    "                                                    fields = (SRC, TRG), root=DATA_DIR)\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
    "\n",
    "# with open(TRAIN_TOKEN_PATH, 'wb') as f:\n",
    "#     pickle.dump(train_data, f)\n",
    "# f.close()\n",
    "# vars(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in request vocabulary: 112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'freqs': Counter({'it': 7,\n",
       "          \"'s\": 4,\n",
       "          'really': 4,\n",
       "          'aggressive': 1,\n",
       "          'to': 18,\n",
       "          'blast': 1,\n",
       "          'obnoxious': 1,\n",
       "          '\"': 2,\n",
       "          'entertainment': 1,\n",
       "          'in': 4,\n",
       "          'your': 6,\n",
       "          'guests': 1,\n",
       "          \"'\": 1,\n",
       "          'faces': 1,\n",
       "          '&': 3,\n",
       "          'amp': 2,\n",
       "          ';': 3,\n",
       "          'they': 1,\n",
       "          'have': 4,\n",
       "          'little': 1,\n",
       "          'recourse': 1,\n",
       "          'and': 9,\n",
       "          'a': 11,\n",
       "          'big': 1,\n",
       "          'bad': 2,\n",
       "          'thing': 2,\n",
       "          'about': 4,\n",
       "          'seriously': 1,\n",
       "          'would': 3,\n",
       "          'pay': 1,\n",
       "          '$': 1,\n",
       "          '30': 1,\n",
       "          'flight': 6,\n",
       "          'for': 8,\n",
       "          'seats': 2,\n",
       "          'that': 5,\n",
       "          'did': 2,\n",
       "          \"n't\": 8,\n",
       "          'this': 6,\n",
       "          'playing': 1,\n",
       "          '.': 27,\n",
       "          'the': 11,\n",
       "          'only': 3,\n",
       "          'flying': 3,\n",
       "          'va': 1,\n",
       "          'yes': 1,\n",
       "          ',': 11,\n",
       "          'nearly': 1,\n",
       "          'every': 1,\n",
       "          'time': 4,\n",
       "          'i': 26,\n",
       "          'fly': 5,\n",
       "          'vx': 1,\n",
       "          '‚Äú': 1,\n",
       "          'ear': 1,\n",
       "          'worm': 1,\n",
       "          '‚Äù': 1,\n",
       "          'wo': 2,\n",
       "          'n‚Äôt': 1,\n",
       "          'go': 1,\n",
       "          'away': 3,\n",
       "          ':)': 1,\n",
       "          'missed': 1,\n",
       "          'prime': 1,\n",
       "          'opportunity': 1,\n",
       "          'men': 1,\n",
       "          'without': 1,\n",
       "          'hats': 1,\n",
       "          'parody': 1,\n",
       "          'there': 1,\n",
       "          'https://t.co/mwpg7grezp': 1,\n",
       "          '@virginamerica': 3,\n",
       "          'well': 1,\n",
       "          \"didn't\": 1,\n",
       "          '‚Ä¶': 1,\n",
       "          'but': 5,\n",
       "          'now': 1,\n",
       "          'do': 4,\n",
       "          '!': 29,\n",
       "          ':-d': 1,\n",
       "          'was': 1,\n",
       "          'amazing': 2,\n",
       "          'arrived': 1,\n",
       "          'an': 2,\n",
       "          'hour': 1,\n",
       "          'early': 1,\n",
       "          'you': 13,\n",
       "          \"'re\": 1,\n",
       "          'too': 1,\n",
       "          'good': 3,\n",
       "          'me': 5,\n",
       "          'know': 3,\n",
       "          'suicide': 1,\n",
       "          'is': 6,\n",
       "          'second': 1,\n",
       "          'leading': 1,\n",
       "          'cause': 2,\n",
       "          'of': 5,\n",
       "          'death': 1,\n",
       "          'among': 1,\n",
       "          'teens': 1,\n",
       "          '10': 1,\n",
       "          '-': 6,\n",
       "          '24': 2,\n",
       "          'lt;3': 1,\n",
       "          'pretty': 1,\n",
       "          'graphics': 1,\n",
       "          'so': 3,\n",
       "          'much': 1,\n",
       "          'better': 1,\n",
       "          'than': 2,\n",
       "          'minimal': 1,\n",
       "          'iconography': 1,\n",
       "          ':d': 1,\n",
       "          'such': 2,\n",
       "          'great': 3,\n",
       "          'deal': 1,\n",
       "          'already': 1,\n",
       "          'thinking': 1,\n",
       "          'my': 16,\n",
       "          '2nd': 1,\n",
       "          'trip': 3,\n",
       "          '@australia': 1,\n",
       "          'even': 1,\n",
       "          'gone': 1,\n",
       "          'on': 7,\n",
       "          '1st': 1,\n",
       "          'yet': 1,\n",
       "          'p': 1,\n",
       "          '@virginmedia': 1,\n",
       "          \"'m\": 3,\n",
       "          '#': 20,\n",
       "          'fabulous': 1,\n",
       "          'seductive': 1,\n",
       "          'skies': 1,\n",
       "          'again': 1,\n",
       "          'u': 1,\n",
       "          'take': 1,\n",
       "          'all': 3,\n",
       "          'stress': 1,\n",
       "          'from': 4,\n",
       "          'travel': 1,\n",
       "          'http://t.co/ahlxhhkiyn': 1,\n",
       "          'thanks': 1,\n",
       "          'sfo': 3,\n",
       "          'pdx': 1,\n",
       "          'schedule': 1,\n",
       "          'still': 1,\n",
       "          'mia': 1,\n",
       "          'excited': 2,\n",
       "          'first': 3,\n",
       "          'cross': 1,\n",
       "          'country': 1,\n",
       "          'lax': 4,\n",
       "          'mco': 1,\n",
       "          \"'ve\": 1,\n",
       "          'heard': 1,\n",
       "          'nothing': 1,\n",
       "          'things': 1,\n",
       "          'virgin': 1,\n",
       "          'america': 2,\n",
       "          '29daystogo': 1,\n",
       "          'flew': 1,\n",
       "          'nyc': 1,\n",
       "          'last': 2,\n",
       "          'week': 2,\n",
       "          'could': 2,\n",
       "          'fully': 1,\n",
       "          'sit': 1,\n",
       "          'seat': 4,\n",
       "          'due': 1,\n",
       "          'two': 1,\n",
       "          'large': 1,\n",
       "          'gentleman': 1,\n",
       "          'either': 1,\n",
       "          'side': 1,\n",
       "          'help': 4,\n",
       "          '‚ù§': 1,\n",
       "          'Ô∏è': 3,\n",
       "          '‚ò∫': 1,\n",
       "          'üëç': 2,\n",
       "          'what': 2,\n",
       "          'be': 4,\n",
       "          'amazingly': 1,\n",
       "          'awesome': 1,\n",
       "          '?': 13,\n",
       "          'bos': 1,\n",
       "          'fll': 1,\n",
       "          'please': 1,\n",
       "          'want': 2,\n",
       "          'with': 6,\n",
       "          'why': 1,\n",
       "          'are': 6,\n",
       "          'fares': 1,\n",
       "          'may': 1,\n",
       "          'over': 1,\n",
       "          'three': 2,\n",
       "          'times': 1,\n",
       "          'more': 1,\n",
       "          'other': 1,\n",
       "          'carriers': 1,\n",
       "          'when': 2,\n",
       "          'available': 1,\n",
       "          'select': 1,\n",
       "          'love': 2,\n",
       "          'graphic': 1,\n",
       "          'http://t.co/ut5grrwaaa': 1,\n",
       "          'hipster': 1,\n",
       "          'innovation': 1,\n",
       "          'feel': 1,\n",
       "          'brand': 1,\n",
       "          'will': 1,\n",
       "          'making': 1,\n",
       "          'bos&gt;las': 1,\n",
       "          'non': 1,\n",
       "          'stop': 1,\n",
       "          'permanently': 1,\n",
       "          'anytime': 1,\n",
       "          'soon': 2,\n",
       "          'guys': 2,\n",
       "          'messed': 1,\n",
       "          'up': 1,\n",
       "          'seating': 2,\n",
       "          '..': 1,\n",
       "          'reserved': 1,\n",
       "          'friends': 1,\n",
       "          'gave': 1,\n",
       "          '...': 1,\n",
       "          'üò°': 1,\n",
       "          'free': 1,\n",
       "          'internet': 1,\n",
       "          'status': 1,\n",
       "          'match': 1,\n",
       "          'program': 1,\n",
       "          ' ': 4,\n",
       "          'applied': 1,\n",
       "          'been': 1,\n",
       "          'weeks': 1,\n",
       "          'called': 1,\n",
       "          'emailed': 1,\n",
       "          'no': 3,\n",
       "          'response': 1,\n",
       "          'happened': 1,\n",
       "          '2': 2,\n",
       "          'ur': 2,\n",
       "          'vegan': 1,\n",
       "          'food': 1,\n",
       "          'options': 1,\n",
       "          'at': 3,\n",
       "          'least': 1,\n",
       "          'say': 1,\n",
       "          'site': 2,\n",
       "          'able': 1,\n",
       "          'eat': 1,\n",
       "          'anything': 1,\n",
       "          'next': 2,\n",
       "          '6': 1,\n",
       "          'hrs': 1,\n",
       "          'fail': 1,\n",
       "          'miss': 1,\n",
       "          'worry': 1,\n",
       "          'we': 2,\n",
       "          \"'ll\": 1,\n",
       "          'together': 1,\n",
       "          'very': 1,\n",
       "          'ca': 2,\n",
       "          'get': 1,\n",
       "          'any': 1,\n",
       "          'cold': 1,\n",
       "          'air': 1,\n",
       "          'vents': 1,\n",
       "          'vx358': 1,\n",
       "          'noair': 1,\n",
       "          'worstflightever': 1,\n",
       "          'roasted': 1,\n",
       "          'sfotobos': 1,\n",
       "          'ewr': 1,\n",
       "          'middle': 2,\n",
       "          'red': 1,\n",
       "          'eye': 1,\n",
       "          'noob': 1,\n",
       "          'maneuver': 1,\n",
       "          'sendambien': 1,\n",
       "          'andchexmix': 1,\n",
       "          'hi': 1,\n",
       "          'just': 3,\n",
       "          'bked': 1,\n",
       "          'cool': 2,\n",
       "          'birthday': 1,\n",
       "          'add': 1,\n",
       "          'elevate': 2,\n",
       "          'entered': 1,\n",
       "          'name': 1,\n",
       "          'during': 1,\n",
       "          'booking': 1,\n",
       "          'problems': 1,\n",
       "          'üò¢': 1,\n",
       "          'hours': 1,\n",
       "          'operation': 1,\n",
       "          'club': 1,\n",
       "          'posted': 1,\n",
       "          'online': 2,\n",
       "          'current': 1,\n",
       "          'left': 1,\n",
       "          'expensive': 1,\n",
       "          'headphones': 1,\n",
       "          '89': 1,\n",
       "          'iad': 1,\n",
       "          'today': 1,\n",
       "          '2a.': 1,\n",
       "          'one': 1,\n",
       "          'answering': 1,\n",
       "          'l&amp;f': 1,\n",
       "          'number': 1,\n",
       "          'awaiting': 1,\n",
       "          'return': 1,\n",
       "          'phone': 1,\n",
       "          'call': 1,\n",
       "          'prefer': 1,\n",
       "          'use': 1,\n",
       "          'self': 1,\n",
       "          'service': 1,\n",
       "          'option': 1,\n",
       "          ':(': 1,\n",
       "          'news': 1,\n",
       "          'start': 1,\n",
       "          'flights': 3,\n",
       "          'hawaii': 2,\n",
       "          'by': 1,\n",
       "          'end': 1,\n",
       "          'year': 1,\n",
       "          'http://t.co/r8p2zy3fe4': 1,\n",
       "          'via': 1,\n",
       "          '@pacificbiznews': 1,\n",
       "          'nice': 1,\n",
       "          'rt': 1,\n",
       "          ':': 3,\n",
       "          'vibe': 1,\n",
       "          'moodlight': 1,\n",
       "          'takeoff': 1,\n",
       "          'touchdown': 1,\n",
       "          'moodlitmonday': 2,\n",
       "          'sciencebehindtheexperience': 1,\n",
       "          'http://t.co/y7o0unxtqp': 1,\n",
       "          'moodlighting': 1,\n",
       "          'way': 1,\n",
       "          'best': 2,\n",
       "          'experience': 1,\n",
       "          'ever': 1,\n",
       "          'calming': 1,\n",
       "          'üíú': 1,\n",
       "          '‚úà': 2,\n",
       "          '@freddieawards': 1,\n",
       "          'done': 2,\n",
       "          'airline': 1,\n",
       "          'around': 1,\n",
       "          'hands': 1,\n",
       "          'down': 1,\n",
       "          'can': 2,\n",
       "          'book': 1,\n",
       "          'chat': 1,\n",
       "          'support': 1,\n",
       "          'not': 1,\n",
       "          'working': 1,\n",
       "          'http://t.co/vhp2gtdwpk': 1,\n",
       "          'view': 1,\n",
       "          'downtown': 1,\n",
       "          'los': 1,\n",
       "          'angeles': 1,\n",
       "          'hollywood': 1,\n",
       "          'sign': 1,\n",
       "          'beyond': 1,\n",
       "          'rain': 1,\n",
       "          'mountains': 1,\n",
       "          'http://t.co/dw5nf0ibtr': 1,\n",
       "          'hey': 1,\n",
       "          'flyer': 1,\n",
       "          'having': 1,\n",
       "          'hard': 1,\n",
       "          'getting': 1,\n",
       "          'added': 1,\n",
       "          'account': 1,\n",
       "          'plz': 1,\n",
       "          'win': 1,\n",
       "          'bid': 1,\n",
       "          'upgrade': 1,\n",
       "          '2/27': 1,\n",
       "          'lax---&gt;sea': 1,\n",
       "          'üç∑': 1,\n",
       "          'üí∫': 1,\n",
       "          'unused': 1,\n",
       "          'ticket': 1,\n",
       "          'moved': 1,\n",
       "          'new': 1,\n",
       "          'city': 1,\n",
       "          'where': 1,\n",
       "          'how': 1,\n",
       "          'before': 1,\n",
       "          'expires': 1,\n",
       "          'travelhelp': 1,\n",
       "          'leaving': 1,\n",
       "          'dallas': 1,\n",
       "          'seattle': 1,\n",
       "          'feb': 1,\n",
       "          'elevategold': 1,\n",
       "          'reason': 1,\n",
       "          'rock': 1,\n",
       "          'dream': 1,\n",
       "          'http://t.co/oa2drfaoq2': 1,\n",
       "          'http://t.co/lwwdac2khx': 1,\n",
       "          'wow': 1,\n",
       "          'blew': 1,\n",
       "          'mind': 1,\n",
       "          '@ladygaga': 3,\n",
       "          '@carrieunderwood': 3,\n",
       "          'after': 1,\n",
       "          'night': 1,\n",
       "          'tribute': 1,\n",
       "          'soundofmusic': 1,\n",
       "          'oscars2015': 1,\n",
       "          'think': 1,\n",
       "          'agree': 1,\n",
       "          'were': 1,\n",
       "          'entertaining': 1}),\n",
       " 'itos': ['<unk>',\n",
       "  '<pad>',\n",
       "  '<sos>',\n",
       "  '<eos>',\n",
       "  '!',\n",
       "  '.',\n",
       "  'i',\n",
       "  '#',\n",
       "  'to',\n",
       "  'my',\n",
       "  '?',\n",
       "  'you',\n",
       "  ',',\n",
       "  'a',\n",
       "  'the',\n",
       "  'and',\n",
       "  'for',\n",
       "  \"n't\",\n",
       "  'it',\n",
       "  'on',\n",
       "  '-',\n",
       "  'are',\n",
       "  'flight',\n",
       "  'is',\n",
       "  'this',\n",
       "  'with',\n",
       "  'your',\n",
       "  'but',\n",
       "  'fly',\n",
       "  'me',\n",
       "  'of',\n",
       "  'that',\n",
       "  ' ',\n",
       "  \"'s\",\n",
       "  'about',\n",
       "  'be',\n",
       "  'do',\n",
       "  'from',\n",
       "  'have',\n",
       "  'help',\n",
       "  'in',\n",
       "  'lax',\n",
       "  'really',\n",
       "  'seat',\n",
       "  'time',\n",
       "  '&',\n",
       "  \"'m\",\n",
       "  ':',\n",
       "  ';',\n",
       "  '@carrieunderwood',\n",
       "  '@ladygaga',\n",
       "  '@virginamerica',\n",
       "  'all',\n",
       "  'at',\n",
       "  'away',\n",
       "  'first',\n",
       "  'flights',\n",
       "  'flying',\n",
       "  'good',\n",
       "  'great',\n",
       "  'just',\n",
       "  'know',\n",
       "  'no',\n",
       "  'only',\n",
       "  'sfo',\n",
       "  'so',\n",
       "  'trip',\n",
       "  'would',\n",
       "  'Ô∏è',\n",
       "  '\"',\n",
       "  '2',\n",
       "  '24',\n",
       "  'amazing',\n",
       "  'america',\n",
       "  'amp',\n",
       "  'an',\n",
       "  'bad',\n",
       "  'best',\n",
       "  'ca',\n",
       "  'can',\n",
       "  'cause',\n",
       "  'cool',\n",
       "  'could',\n",
       "  'did',\n",
       "  'done',\n",
       "  'elevate',\n",
       "  'excited',\n",
       "  'guys',\n",
       "  'hawaii',\n",
       "  'last',\n",
       "  'love',\n",
       "  'middle',\n",
       "  'moodlitmonday',\n",
       "  'next',\n",
       "  'online',\n",
       "  'seating',\n",
       "  'seats',\n",
       "  'site',\n",
       "  'soon',\n",
       "  'such',\n",
       "  'than',\n",
       "  'thing',\n",
       "  'three',\n",
       "  'ur',\n",
       "  'want',\n",
       "  'we',\n",
       "  'week',\n",
       "  'what',\n",
       "  'when',\n",
       "  'wo',\n",
       "  '‚úà',\n",
       "  'üëç'],\n",
       " 'unk_index': 0,\n",
       " 'stoi': defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7fdd61311e48>>,\n",
       "             {'<unk>': 0,\n",
       "              '<pad>': 1,\n",
       "              '<sos>': 2,\n",
       "              '<eos>': 3,\n",
       "              '!': 4,\n",
       "              '.': 5,\n",
       "              'i': 6,\n",
       "              '#': 7,\n",
       "              'to': 8,\n",
       "              'my': 9,\n",
       "              '?': 10,\n",
       "              'you': 11,\n",
       "              ',': 12,\n",
       "              'a': 13,\n",
       "              'the': 14,\n",
       "              'and': 15,\n",
       "              'for': 16,\n",
       "              \"n't\": 17,\n",
       "              'it': 18,\n",
       "              'on': 19,\n",
       "              '-': 20,\n",
       "              'are': 21,\n",
       "              'flight': 22,\n",
       "              'is': 23,\n",
       "              'this': 24,\n",
       "              'with': 25,\n",
       "              'your': 26,\n",
       "              'but': 27,\n",
       "              'fly': 28,\n",
       "              'me': 29,\n",
       "              'of': 30,\n",
       "              'that': 31,\n",
       "              ' ': 32,\n",
       "              \"'s\": 33,\n",
       "              'about': 34,\n",
       "              'be': 35,\n",
       "              'do': 36,\n",
       "              'from': 37,\n",
       "              'have': 38,\n",
       "              'help': 39,\n",
       "              'in': 40,\n",
       "              'lax': 41,\n",
       "              'really': 42,\n",
       "              'seat': 43,\n",
       "              'time': 44,\n",
       "              '&': 45,\n",
       "              \"'m\": 46,\n",
       "              ':': 47,\n",
       "              ';': 48,\n",
       "              '@carrieunderwood': 49,\n",
       "              '@ladygaga': 50,\n",
       "              '@virginamerica': 51,\n",
       "              'all': 52,\n",
       "              'at': 53,\n",
       "              'away': 54,\n",
       "              'first': 55,\n",
       "              'flights': 56,\n",
       "              'flying': 57,\n",
       "              'good': 58,\n",
       "              'great': 59,\n",
       "              'just': 60,\n",
       "              'know': 61,\n",
       "              'no': 62,\n",
       "              'only': 63,\n",
       "              'sfo': 64,\n",
       "              'so': 65,\n",
       "              'trip': 66,\n",
       "              'would': 67,\n",
       "              'Ô∏è': 68,\n",
       "              '\"': 69,\n",
       "              '2': 70,\n",
       "              '24': 71,\n",
       "              'amazing': 72,\n",
       "              'america': 73,\n",
       "              'amp': 74,\n",
       "              'an': 75,\n",
       "              'bad': 76,\n",
       "              'best': 77,\n",
       "              'ca': 78,\n",
       "              'can': 79,\n",
       "              'cause': 80,\n",
       "              'cool': 81,\n",
       "              'could': 82,\n",
       "              'did': 83,\n",
       "              'done': 84,\n",
       "              'elevate': 85,\n",
       "              'excited': 86,\n",
       "              'guys': 87,\n",
       "              'hawaii': 88,\n",
       "              'last': 89,\n",
       "              'love': 90,\n",
       "              'middle': 91,\n",
       "              'moodlitmonday': 92,\n",
       "              'next': 93,\n",
       "              'online': 94,\n",
       "              'seating': 95,\n",
       "              'seats': 96,\n",
       "              'site': 97,\n",
       "              'soon': 98,\n",
       "              'such': 99,\n",
       "              'than': 100,\n",
       "              'thing': 101,\n",
       "              'three': 102,\n",
       "              'ur': 103,\n",
       "              'want': 104,\n",
       "              'we': 105,\n",
       "              'week': 106,\n",
       "              'what': 107,\n",
       "              'when': 108,\n",
       "              'wo': 109,\n",
       "              '‚úà': 110,\n",
       "              'üëç': 111}),\n",
       " 'vectors': None}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the vocabulary from the training set\n",
    "# each word is represented by a unique number(maybe it's the order they appear in the corpus?)\n",
    "# min_freq = 2: a valid word must appear at least twice\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)\n",
    "print(f\"Unique tokens in request vocabulary: {len(TRG.vocab)}\")\n",
    "vars(TRG.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into mini batches\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# use GPU if possible. cuda is not available on most Mac GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# create iterators that iterate through the batches\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm using cpu\n",
      "The model has 7,471,216 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "print(\"I'm using\", device)\n",
    "\n",
    "# create a model\n",
    "INPUT_DIM = len(SRC.vocab)  # each word is represented with a one-hot vector, which has teh length of the vocab size\n",
    "OUTPUT_DIM = len(TRG.vocab)  \n",
    "ENC_EMB_DIM = 256  # each word will be compressed to EMB_DIM by embedding matrices\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512  # sentences of different length will be converted into vecters of fixed length of HID_DIM\n",
    "N_LAYERS = 2  # use a multilayer RNN\n",
    "ENC_DROPOUT = 0.5  # randomly drop the value for some nodes\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "# instantiate encoder, decoder, and seq2seq networks\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "model.apply(init_weights)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "# define the optimizer as Adam\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# loss function\n",
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 4.727 | Train PPL: 112.981\n",
      "\t Val. Loss: 4.453 |  Val. PPL:  85.922\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "N_EPOCHS = 1  # number of epochs the seq2seq network is going to train through the entier dataset\n",
    "CLIP = 1\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):  \n",
    "    start_time = time.time()\n",
    "   \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss, _ = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # save the model that generates the best validation error\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save (model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    # report the status\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 4.616 | Test PPL: 101.055 |\n",
      "The shape of the context vector set is torch.Size([2, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "# generate the context vectors for the test set\n",
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "test_loss, context_vectors = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')\n",
    "print('The shape of the context vector set is', context_vectors.shape)\n",
    "\n",
    "# save the context vectors to disk\n",
    "with open(CONTEXT_VECTORS_PATH, 'wb') as f:\n",
    "    pickle.dump(context_vectors, f)\n",
    "f.close()\n",
    "\n",
    "# # load the context vectors from disk\n",
    "# context_vectors = pickle.load(open(CONTEXT_VECTORS_PATH, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some highlights in our experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2, 50, 49,  ...,  1,  1,  1],\n",
      "        [ 2,  0,  0,  ...,  1,  1,  1],\n",
      "        [ 2,  0,  4,  ...,  0,  0,  3],\n",
      "        ...,\n",
      "        [ 2,  6,  0,  ...,  1,  1,  1],\n",
      "        [ 2,  0, 24,  ...,  1,  1,  1],\n",
      "        [ 2,  0, 11,  ...,  1,  1,  1]])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_iterator):\n",
    "    if i == 0:\n",
    "        print(batch.src.transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 3],\n",
      "        [3, 4]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2],[3,4]])\n",
    "b = a\n",
    "c = a.data.clone()\n",
    "a[0][1] = 3\n",
    "a = torch.tensor([[0,0],[0,0]])\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vars(train_data.examples[0])['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(train_iterator.dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.context_vectors[0][1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.context_vectors[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
